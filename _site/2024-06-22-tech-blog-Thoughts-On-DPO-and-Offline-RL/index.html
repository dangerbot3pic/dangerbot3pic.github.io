<!DOCTYPE html>
<html lang="en">
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2020 -->
  <head>
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Thoughts on DPO and Offline RL</title>
  
  
  <meta name="author" content="Padmanaba Srinivasan">
  
  
  

  <link rel="alternate" type="application/rss+xml" title="Padmanaba Srinivasan - Computer Science PhD Student" href="http://localhost:4000/feed.xml">

  

  

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-172123502-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-172123502-1');
</script>



  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/main.css">
    
  

  

  

  <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="Thoughts on DPO and Offline RL">
  

   
  <meta property="og:description" content="Direct Preference Optimization is all the rage now in LLMs, and rightly so! The derivation is neat (and very familiar to those experienced with reinforcement learning) and allows direct, preference-based finetuning of regression-trained LLMs without having to learn a reward model. In this post, I want to explore what implications...">
  


  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Padmanaba Srinivasan">
  <meta property="og:article:published_time" content="2024-06-22T00:00:00+01:00">
  

  
  <meta property="og:url" content="http://localhost:4000/2024-06-22-tech-blog-Thoughts-On-DPO-and-Offline-RL/">
  <link rel="canonical" href="http://localhost:4000/2024-06-22-tech-blog-Thoughts-On-DPO-and-Offline-RL/">
  
  

  
  <meta property="og:image" content="http://localhost:4000/assets/img/path.jpg">
  


  <!-- Twitter summary cards -->
  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  
  <meta name="twitter:title" content="Thoughts on DPO and Offline RL">
  

  
  <meta name="twitter:description" content="Direct Preference Optimization is all the rage now in LLMs, and rightly so! The derivation is neat (and very familiar to those experienced with reinforcement learning) and allows direct, preference-based finetuning of regression-trained LLMs without having to learn a reward model. In this post, I want to explore what implications...">
  

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/path.jpg">
  

  

  

</head>


  <body>

    

  
    <nav class="navbar navbar-expand-md navbar-light fixed-top navbar-custom "><a class="navbar-brand" href="http://localhost:4000/">Padmanaba Srinivasan</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/cv/CV.pdf">CV/Resume</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/aboutme">About Me</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/techblog">Blog</a>
          </li></ul>
  </div>

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navbar avatar" class="avatar-img" src="/assets/img/park.jpg" />
        </a>
      </div>
    </div>
  

</nav>


    <!-- TODO this file has become a mess, refactor it -->






  <div id="header-big-imgs" data-num-img=1
    
    
    
      
      data-img-src-1="http://localhost:4000/assets/img/path.jpg"
    
    
    
  ></div>


<header class="header-section has-img">

<div class="big-img intro-header">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Thoughts on DPO and Offline RL</h1>
      
      
      
          <span class="post-meta">Posted on 22 June, 2024</span>
          
      
        </div>
      </div>
    </div>
  </div>
  <span class='img-desc'></span>
</div>

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Thoughts on DPO and Offline RL</h1>
      
      
      
          <span class="post-meta">Posted on 22 June, 2024</span>
          
      
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class="container-md">
  <div class="row">
    <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">

      

      <article role="main" class="blog-post">
        <p><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization</a> is all the rage now in LLMs, and rightly so! The derivation is neat (and very familiar to those experienced with reinforcement learning) and allows direct, preference-based finetuning of regression-trained LLMs without having to learn a reward model.</p>

<p>In this post, I want to explore what implications a DPO-style training can offer to offline RL. To this end, I begin with the derivation.</p>

<p>Consider the following optimization problem:</p>

\[max_{\pi}\quad  \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [r(s, a)]\quad s.t.\quad D_{\text{KL}} [\pi (a | s) \mid\mid \pi_{\text{ref}} (a | s)] \leq \epsilon,\]

<p>which maximizes a reward function while constraining the KL–divergence between the current policy and a reference policy. Rewriting the objective using the Lagrangian multiplier \(\beta\), we get:</p>

\[max_{\pi}\quad  \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [r(s, a)] - \beta D_{\text{KL}} (\pi(a | s) \mid\mid \pi_{\text{ref}} (a | s))
    \\
    = max_{\pi}\quad \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [r(s, a) - \beta \log \frac{\pi(s | a)}{\pi_{\text{ref}} (s | a)}]
    \\
    = min_{\pi}\quad \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [\log \frac{\pi(s | a)}{\pi_{\text{ref}} (s | a)} - \frac{1}{\beta} r(s, a)]
    \\
    = min_{\pi}\quad \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [ \log \frac{\pi(a | s)}{\pi_{\text{ref}} (a | s) \exp (\frac{1}{\beta} r(s, a)) \frac{1}{Z(s)}}  - \log Z(s)],\]

<p>where</p>

\[Z(s) = \int_{\mathcal{A}} \pi_{\text{ref}} (a | s) \exp(\frac{1}{\beta} r(s, a)),\]

<p>is the state-dependent partition function.</p>

<p>The (estimated) reward can then be expressed as:</p>

\[r(s, a) = \beta \log \frac{\pi(a | s)}{\pi_{\text{ref}} (a | s)} + \beta \log Z(s)\]

<p>This poses a problem as \(Z(s)\) requires integrating over actions to compute – in practice this is intractable for continuous \(\mathcal{A}\). Most prior methods tends to ignore normalizing constant and attempts to approximate are rendered both <a href="https://arxiv.org/abs/2006.09359">more computationally expensive and less performant</a>.</p>

<p>The authors of DPO notice that rather than optimizing the objective directly, they can instead optimize the difference:</p>

\[r(s, a_1) - r(s, a_2) = (\beta \log \frac{\pi(a_1 | s)}{\pi_{\text{ref}} (a_1 | s)} + \beta \log Z(s)) - (\beta \log \frac{\pi(a_2 | s)}{\pi_{\text{ref}} (a_2 | s)} + \beta \log Z(s))
    \\
    = \beta (\log \frac{\pi(a_1 | s)}{\pi_{\text{ref}} (a_1 | s)} - \log \frac{\pi(a_2 | s)}{\pi_{\text{ref}} (a_2 | s)}),\]

<p>where the partition functions cancel out, resulting in a direct optimizion of log-probabilities. When training using preference-annotated data, the preference oracle denotes (absolute) paired preferences \(p(a_1 \succ a_2)\) that can be directly regressed via a <a href="https://en.wikipedia.org/wiki/Bradley–Terry_model">Bradley-Terry preference model</a> to yield the DPO objective.</p>

<p><strong>What does this mean for offline RL?</strong></p>

<p>First off, DPO is a <em>finetuning</em> process that requires the base policy \(\pi_{\text{ref}}\) to be a pretrained LLM – this can be a simple MLE-trained model – that is nudged to generate human-preferred sequences from an offline buffer. At no point is are samples drawn from \(\pi\) and instead, \(a_1\) and \(a_2\) are drawn from \(\pi_{\text{ref}}\). This is distinctly different from an RLHF-based approach that learns a reward model and requires humans to preference-rank samples drawn from a series of policies. Consequently, DPO is a far simpler and more stable approach to training LLMs.</p>

<p>Despite the <em>neatness</em> of DPO, by limiting the algorithm of offline finetuning based only on samples from a maximum likelihood model \(\pi_{\text{ref}}\) seems like it might limit the extent of improvement (or, to put it another way, human-preferred alignment) and this could be the case for many subsequent <a href="https://arxiv.org/pdf/2402.05749">offline refinement methods</a>.</p>

<p>Does DPO have any bearing on offline RL? Simply put, yes: several preference-inspired offline RL methods (for continuous control) have been proposed, such as (non-exhaustive):</p>

<ol>
  <li>Inverse-RL-inspired <a href="https://arxiv.org/abs/2305.15363">IPL</a> which learns a preference-based reward function and subsequently trains an RL policy.</li>
  <li><a href="https://arxiv.org/abs/2107.09251">OPAL</a> trains a policy on high-reward-seeking preferences between subtrajectories (following the behavioral policy).</li>
  <li><a href="https://arxiv.org/abs/2301.12842">DPPO</a> directly uses human-labelled datasets to train human-aligned policies.</li>
  <li><a href="https://arxiv.org/abs/2305.16217">OPPO</a> and <a href="https://arxiv.org/abs/2303.00957">PT</a> predict entire  to align with human-preferred trajectories.</li>
</ol>

<p>A commonality between these methods is their reliance on on-policy trajactories/value-estimation. A key driver of the development of offline RL methods is the ability to enable <em>stitching</em> together of suboptimal (behavior-policy-generated) trajectories to improve on the behavior policy. I have yet to come across any preference-inspired methods that can move beyond on the on-policy setting (and is perhaps why I haven’t seen much \(\texttt{antmaze}\) evaluation).</p>

<p>The DPO trick also brings other challenges for offline RL: in practice we want to use a Q function to estimate rewards with which the <strong>deadly triad</strong> can be notoriously unstable and overestimate values for OOD actions. We also need to approximate \(\pi_{\text{ref}}\) beyond the challenges of the quality of approximation, we do not know whether the behavior policy is multimodal (or how multimodal it is) which often necessitates VAEs (which produces explicit density estimates that can only be extracted by sampling) or MDNs (which have their own, set of training challenges + need to know how multimodal a behavior policy is beforehand).</p>

<p>Assuming that we have solved the aformentioned challenges, offline RL still poses yet another challenges: multimodal behavior policies can produce multimodal reward functions. DPO (and -like) objectives assume we can sample \(a_1, a_2\) such that \(a_1 \succ a_2\), but real-world offline datasets offer no such guarantee of paired-ness. Prior preference-based methods conventiently overcome this challenge by operating at the trajectory level or by using specific, preference-annotated datasets.</p>

<p>The key point I want to make in this, now long-winded post, is that the DPO tricks could enable a very interesting set of approaches to offline RL, provided we can overcome the fundamental challenges of value estimation, density estimation and limitations of on-policy evaluation.</p>

      </article>

      
        <div class="blog-tags">
          Tags:
          
          
            <a href="/tags#tech">tech</a>
          
            <a href="/tags#machine learning">machine learning</a>
          
            <a href="/tags#reinforcement learning">reinforcement learning</a>
          
          
        </div>
      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  
    <a href="https://twitter.com/intent/tweet?text=Thoughts+on+DPO+and+Offline+RL&url=http%3A%2F%2Flocalhost%3A4000%2F2024-06-22-tech-blog-Thoughts-On-DPO-and-Offline-RL%2F"
      class="btn btn-social-icon btn-twitter" title="Share on Twitter">
      <span class="fab fa-fw fa-twitter" aria-hidden="true"></span>
      <span class="sr-only">Twitter</span>
    </a>
  

  
    <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2024-06-22-tech-blog-Thoughts-On-DPO-and-Offline-RL%2F"
      class="btn btn-social-icon btn-facebook" title="Share on Facebook">
      <span class="fab fa-fw fa-facebook" aria-hidden="true"></span>
      <span class="sr-only">Facebook</span>
    </a>
  

  
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2024-06-22-tech-blog-Thoughts-On-DPO-and-Offline-RL%2F"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fab fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

  

</section>



      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2024-02-22-tech-blog-Some-Interesting-Offline-RL-Methods-Early-2024/" data-toggle="tooltip" data-placement="top" title="Some Interesting Offline RL Methods (Early 2024)">&larr; Previous Post</a>
        </li>
        
        
      </ul>
              
  
  
  

  



    </div>
  </div>
</div>


    <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:padmanaba.srinivasan16@imperial.ac.uk" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/dangerbot3pic" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/padmanaba-srinivasan-b67bb1137" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://steamcommunity.com/id/dangerbot3pic" title="Steam">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-steam fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Steam</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Padmanaba Srinivasan
        &nbsp;&bull;&nbsp;
      
      2024

      

      
      </p>
      <!-- Please don't remove this, keep my open source work credited :) -->
      <p class="theme-by text-muted">
        Theme by
        <a href="https://beautifuljekyll.com">beautiful-jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>

  
    
  
    
  <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/main.js"></script>
    
  






  
  </body>
</html>
