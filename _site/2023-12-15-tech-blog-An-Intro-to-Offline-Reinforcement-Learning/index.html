<!DOCTYPE html>
<html lang="en">
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2020 -->
  <head>
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>An Intro to Offline Reinforcement Learning</title>
  
  
  <meta name="author" content="Padmanaba Srinivasan">
  
  
  

  <link rel="alternate" type="application/rss+xml" title="Padmanaba Srinivasan - Computer Science PhD Student" href="http://localhost:4000/feed.xml">

  

  

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-172123502-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-172123502-1');
</script>



  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/main.css">
    
  

  

  

  <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="An Intro to Offline Reinforcement Learning">
  

   
  <meta property="og:description" content="What is Reinforcement Learning? Say you have bought a table from a certain Swedish retailer famous for both their fairly priced furnite, and meatballs, but have lost the instructions to assemble it. A trivial, albeit cumbersome approach would be to just try putting different parts together and to make it...">
  


  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Padmanaba Srinivasan">
  <meta property="og:article:published_time" content="2023-12-15T00:00:00+00:00">
  

  
  <meta property="og:url" content="http://localhost:4000/2023-12-15-tech-blog-An-Intro-to-Offline-Reinforcement-Learning/">
  <link rel="canonical" href="http://localhost:4000/2023-12-15-tech-blog-An-Intro-to-Offline-Reinforcement-Learning/">
  
  

  
  <meta property="og:image" content="http://localhost:4000/assets/img/path.jpg">
  


  <!-- Twitter summary cards -->
  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  
  <meta name="twitter:title" content="An Intro to Offline Reinforcement Learning">
  

  
  <meta name="twitter:description" content="What is Reinforcement Learning? Say you have bought a table from a certain Swedish retailer famous for both their fairly priced furnite, and meatballs, but have lost the instructions to assemble it. A trivial, albeit cumbersome approach would be to just try putting different parts together and to make it...">
  

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/path.jpg">
  

  

  

</head>


  <body>

    

  
    <nav class="navbar navbar-expand-md navbar-light fixed-top navbar-custom "><a class="navbar-brand" href="http://localhost:4000/">Padmanaba Srinivasan</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/cv/CV.pdf">CV/Resume</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/aboutme">About Me</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/techblog">Blog</a>
          </li></ul>
  </div>

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navbar avatar" class="avatar-img" src="/assets/img/park.jpg" />
        </a>
      </div>
    </div>
  

</nav>


    <!-- TODO this file has become a mess, refactor it -->






  <div id="header-big-imgs" data-num-img=1
    
    
    
      
      data-img-src-1="http://localhost:4000/assets/img/path.jpg"
    
    
    
  ></div>


<header class="header-section has-img">

<div class="big-img intro-header">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>An Intro to Offline Reinforcement Learning</h1>
      
      
      
          <span class="post-meta">Posted on 15 December, 2023</span>
          
      
        </div>
      </div>
    </div>
  </div>
  <span class='img-desc'></span>
</div>

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>An Intro to Offline Reinforcement Learning</h1>
      
      
      
          <span class="post-meta">Posted on 15 December, 2023</span>
          
      
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class="container-md">
  <div class="row">
    <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">

      

      <article role="main" class="blog-post">
        <h2 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h2>

<p>Say you have bought a table from a certain Swedish retailer famous for both their fairly priced furnite, and meatballs, but have lost the instructions to assemble it. A trivial, albeit cumbersome approach would be to just try putting different parts together and to make it look more and more like the image on the box. Through repeated attempts, you learn about how different parts fit together until mastery, at which point you are rewarded with a completed table.</p>

<p>This process of repetition and feedback describes the fundamental process of Reinforcement Learning (RL). More abstractly, in RL we aim to train an agent to maximize a reward by interacting with the environment. This is usually achieved by allowing the agent to take actions and receive a reward as feedback and repeating this process until it learns an <em>optimal policy</em> to solve the task.</p>

<p>Rephrasing all this more formally, the agent follows a policy \(\pi \in \mathcal{\Pi}\) in an <strong>environment</strong> by selecting an action \(a \in \mathcal{A}\) based on the current state \(s \in \mathcal{S}\). Once an action is taken, the environment produces a <strong>reward</strong> \(r \in \mathcal{R}\) which is used as feedback to the agent. By executing an action, the agent is able to effect a change in the environment that causes a transition to a new state \(s' \in \mathcal{S}\). The way in which environment dynamics work is defined by a <strong>model</strong>, which may be known or approximated allowing the use of model-based RL algorithms, or unknown in which case model-free RL algorithms must be used.</p>

<p>The agent’s goal is to learn a policy \(\pi(s)\) that maximizes the expected future reward and is taught to do so using a value function \(V(s)\) that predicts the expected future reward from that state following the policy \(\pi\).</p>

<h2 id="from-rl-to-offline-rl">From RL to Offline RL</h2>

<p>The agent needs to be trained on data collected by interacting with the environment over a series of time steps \(1, 2, ..., T\) expressed as a trajectory: \(S_1, A_1, r_1, S_2, A_2, r_2, ..., S_T, A_T, r_T\). <strong>On-Policy</strong> RL generates trajectories using the current policy \(\pi_k\) and learns from this to produce the next policy \(\pi_{k+1}\) to produce a sequence of policies that successively improve. <strong>Off-policy</strong> RL algorithms generate some trajectories using the current policy and
store trajectories from past policies in a replay buffer. Samples from the replay buffer therefore contain samples from a mixture of past policies and these are sampled to produce the next policy \(\pi_{k+1}\). The replay buffer can be updated periodically to add new experiences and remove old ones. In the <strong>Offline</strong> RL setting, the alrogithm is further constrained to using a fixed dataset of trajectories collected produced by (potentially unknown) behavioral policies, with no ability to interact with and explore the environment.</p>

<p>In the offline domain, only the values of the actions present in the dataset can be empirically learned. Standard off-policy RL algorithms have no way of evaluating and exploring the how good out-of-distribution (OOD) actions and so, the neural networks used to learn the value will extrapolate in OOD regions and <a href="https://offline-rl-neurips.github.io/pdf/41.pdf">overestimate their value</a> that the trained policy may select. In reality these actions may be quite poor, so when the policy is deployed in the environment it will execute these untrusted actions and perform poorly.</p>

<p>Clearly, in order to learn effectively online, an offline RL algorithm must both learn in a similar way to off-policy algorithms while also either:</p>

<ol>
  <li>Directly address the extrapolation problem and actively push down the values of OOD actions;</li>
  <li>Constrain the actor to select those actions present in the dataset.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>Standard online deep learning methods have achieved incredible success in recent years, from playing <a href="https://en.wikipedia.org/wiki/AlphaGo">Go</a> to <a href="https://en.wikipedia.org/wiki/ChatGPT">conversational agents</a> which learn by interactng with the environment. In many domains though, it may be expensive or impossible to directly interact with the environment such as when training <a href="https://arxiv.org/abs/2110.07067">self-driving cars</a>, <a href="https://arxiv.org/abs/2105.01006">robotic surgeons</a> (including <a href="https://arxiv.org/pdf/2109.02323.pdf">safe methods</a> and other <a href="https://arxiv.org/abs/2002.03478">clinical</a> applications) and <a href="https://www.ijcai.org/proceedings/2020/0464.pdf">sports modelling</a>. Offline RL algorithms can learn from demonstrations produced by humans, that are potentially suboptimal and “stitch” together optimal trajectories from suboptimal data in a way that is <a href="https://arxiv.org/abs/2106.04895">sample efficient</a>.</p>

<p>I plan to do a more comprehensive <a href="https://arxiv.org/abs/2203.01387">review</a> of current offline RL methods as there have been many exciting developments in 2023 that have seen notable success; yet many of these methods also exhibit some disappointing trends in offline RL. I will cover some of of the most interesting papers of the year in my next post and my thoughts on each approach.</p>

      </article>

      
        <div class="blog-tags">
          Tags:
          
          
            <a href="/tags#tech">tech</a>
          
            <a href="/tags#machine learning">machine learning</a>
          
            <a href="/tags#reinforcement learning">reinforcement learning</a>
          
          
        </div>
      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  
    <a href="https://twitter.com/intent/tweet?text=An+Intro+to+Offline+Reinforcement+Learning&url=http%3A%2F%2Flocalhost%3A4000%2F2023-12-15-tech-blog-An-Intro-to-Offline-Reinforcement-Learning%2F"
      class="btn btn-social-icon btn-twitter" title="Share on Twitter">
      <span class="fab fa-fw fa-twitter" aria-hidden="true"></span>
      <span class="sr-only">Twitter</span>
    </a>
  

  
    <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2023-12-15-tech-blog-An-Intro-to-Offline-Reinforcement-Learning%2F"
      class="btn btn-social-icon btn-facebook" title="Share on Facebook">
      <span class="fab fa-fw fa-facebook" aria-hidden="true"></span>
      <span class="sr-only">Facebook</span>
    </a>
  

  
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2023-12-15-tech-blog-An-Intro-to-Offline-Reinforcement-Learning%2F"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fab fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

  

</section>



      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2021-06-16-tech-blog-A-Summary-of-Action-Recognition-So-Far/" data-toggle="tooltip" data-placement="top" title="A Summary of Action Recognition So Far">&larr; Previous Post</a>
        </li>
        
        
      </ul>
              
  
  
  

  



    </div>
  </div>
</div>


    <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:padmanaba.srinivasan16@imperial.ac.uk" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/dangerbot3pic" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/padmanaba-srinivasan-b67bb1137" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://steamcommunity.com/id/dangerbot3pic" title="Steam">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-steam fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Steam</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Padmanaba Srinivasan
        &nbsp;&bull;&nbsp;
      
      2024

      

      
      </p>
      <!-- Please don't remove this, keep my open source work credited :) -->
      <p class="theme-by text-muted">
        Theme by
        <a href="https://beautifuljekyll.com">beautiful-jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>

  
    
  
    
  <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/main.js"></script>
    
  






  
  </body>
</html>
