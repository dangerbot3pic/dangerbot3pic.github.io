<!DOCTYPE html>
<html lang="en">
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2020 -->
  <head>
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Starting out with Gaussian Processes</title>
  
  
  <meta name="author" content="Padmanaba Srinivasan">
  
  
  
  <meta name="description" content="A Gaussian Process Primer">
  

  <link rel="alternate" type="application/rss+xml" title="Padmanaba Srinivasan - Computer Science PhD Student" href="http://localhost:4000/feed.xml">

  

  

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-172123502-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-172123502-1');
</script>

-->

  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/main.css">
    
  

  

  

  <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="Starting out with Gaussian Processes">
  

   
  <meta property="og:description" content="A Gaussian Process Primer">
  


  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Padmanaba Srinivasan">
  <meta property="og:article:published_time" content="2020-08-25T00:00:00+01:00">
  

  
  <meta property="og:url" content="http://localhost:4000/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/">
  <link rel="canonical" href="http://localhost:4000/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/">
  
  

  
  <meta property="og:image" content="http://localhost:4000/assets/img/path.jpg">
  


  <!-- Twitter summary cards -->
  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  
  <meta name="twitter:title" content="Starting out with Gaussian Processes">
  

  
  <meta name="twitter:description" content="A Gaussian Process Primer">
  

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/path.jpg">
  

  

  

</head>


  <body>

    

  
    <nav class="navbar navbar-expand-md navbar-light fixed-top navbar-custom "><a class="navbar-brand" href="http://localhost:4000/">Padmanaba Srinivasan</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/cv/CV.pdf">CV/Resume</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/aboutme">About Me</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/techblog">Blog</a>
          </li></ul>
  </div>

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navbar avatar" class="avatar-img" src="/assets/img/park.jpg" />
        </a>
      </div>
    </div>
  

</nav>


    <!-- TODO this file has become a mess, refactor it -->






  <div id="header-big-imgs" data-num-img=1
    
    
    
      
      data-img-src-1="http://localhost:4000/assets/img/path.jpg"
    
    
    
  ></div>


<header class="header-section has-img">

<div class="big-img intro-header">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Starting out with Gaussian Processes</h1>
      
        
      <h2 class="post-subheading">A Gaussian Process Primer</h2>
      
      
      
      
          <span class="post-meta">Posted on 25 August, 2020</span>
          
      
        </div>
      </div>
    </div>
  </div>
  <span class='img-desc'></span>
</div>

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Starting out with Gaussian Processes</h1>
      
        
      <h2 class="post-subheading">A Gaussian Process Primer</h2>
      
      
      
      
          <span class="post-meta">Posted on 25 August, 2020</span>
          
      
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class="container-md">
  <div class="row">
    <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">

      

      <article role="main" class="blog-post">
        <p>After taking the course, Mathematics for Machine Learning (MML), at university and reading the excellent book of the <a href="https://mml-book.com/">same name</a>, I was well and truly hooked on Gaussian Processes. The <a href="https://mml-book.com/">MML</a> book introduces the idea of applying the full Bayesian treatment to Linear Regression, leading to the simple and beautiful Bayesian Linear Regression. The book, <a href="http://www.gaussianprocess.org/gpml/">Gaussian Processes for Machine Learning</a> (GPML), tackles Gaussian Processes in much more detail and the rest of this post leans extensively on the material covered in <a href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf">Chapter 2</a>. A great way to get started with the intuition and theory behind Gaussian Processes is to read Chapter 9 of the <a href="https://mml-book.com/">MML</a> book, after covering Chapters 1-7 which runs through the required mathematics.</p>

<p>What is a Gaussian Process? In a nutshell, any possible number of functions can explain a set of observed data and Gaussian Processes assign a probability to each of these possible functions. This results in a probability distribution which allows us to make point estimates in addition to characterising the uncertainty of a prediction.</p>

<p>With that out of the way, letâ€™s get started.</p>

<h3 id="generating-data">Generating Data</h3>

<p>In the real-world data can be generated by any function. This function is usually unknown and the role of machine learning is to try and find this function using observations. To make matters more complicated real-world data is often noisy. We model the function of the observations as <script type="math/tex">y = f(x) + \epsilon</script> where <script type="math/tex">f(x)</script> is the underyling function generating data in the real-world and <script type="math/tex">\epsilon</script> is a noise term, <script type="math/tex">\epsilon \sim N(0, \sigma_n^2)</script>. Note that <script type="math/tex">f(x)</script> is not necessarily a linear function. We consider the data generating function <script type="math/tex">f(x) = sin(2\pi x) + cos(9\pi x)</script>.</p>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/f_x.png" alt="Function $$ f(x) = sin(2\pi x) + cos(9\pi x) $$" />
<em>Function <script type="math/tex">f(x) = sin(2\pi x) + cos(9\pi x)</script></em></p>

<p>We obtain 1000 observations, but this is unrepresentative of real-world data; we still need to add some noise. We set the standard deviation of the noise term, <script type="math/tex">\sigma_n = 0.4</script> and sample the distribution for <script type="math/tex">\epsilon</script> 1000 times and add these to the values of <script type="math/tex">f(x)</script>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">9</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sigma_n</span> <span class="o">=</span> <span class="mf">0.4</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/y_plot.png" alt="$$ y = f(x) + \epsilon $$" />
<em><script type="math/tex">y = f(x) + \epsilon</script></em></p>

<p>The choice of <script type="math/tex">\sigma_n</script> determines how noisy the observations are. The errors, that is, the differences between the observations and the function <script type="math/tex">f(x)</script> should be normally distributed. We can verify this visually by plotting the distribution of errors.</p>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/error_dist.png" alt="Error Distribution" />
<em>Error Distribution</em></p>

<h3 id="kernel-functions">Kernel Functions</h3>

<p>A Gaussian Process is entirely defined by itsâ€™ mean and kernel functions. For this post, we look only at the kernel function. In the case of Bayesian Linear Regression, a linear kernel is applied, allowing the model to find linear relationships between the covariates and outcomes. <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">The Kernel Cookbook</a> by <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a> provides an excellent explanation of the different types of Kernel Functions available and their implications. We implement the Squared Exponential Kernel and the Rational Quadratic Kernel.</p>

<script type="math/tex; mode=display">\begin{align}
        K_{SE}(x, x') = \sigma_f^2 exp(-\frac{(x-x')^2}{2l^2})
    \end{align}</script>

<script type="math/tex; mode=display">\begin{align}
        K_{RQ}(x, x') = \sigma_f^2 (1+\frac{(x-x')^2}{2\alpha l^2})^{-\alpha}
    \end{align}</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>    <span class="k">def</span> <span class="nf">kernel_squared_exponential</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">sigma_f</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span> <span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="p">)</span>
        <span class="k">return</span> <span class="n">K</span>

    <span class="k">def</span> <span class="nf">kernel_rational_quadratic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="s">""" As alpha -&gt; infinity, we get the squared exponential kernel """</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">sigma_f</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">((</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="p">),</span> <span class="o">-</span><span class="n">alpha</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">K</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="the-path-to-inference">The Path to Inference</h3>

<p>Generally, we are not interested in functions drawn from the prior as arenâ€™t conditioned on any data. What we are interested in, however, is how to obtain the distribution of the posterior and an important step in that process is to characterise the joint distribution of training and test points.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
        \begin{bmatrix}
        f \\
        f_*
        \end{bmatrix}
        &\sim \mathcal{N}(
            \begin{bmatrix}
        \mu_f \\
        \mu_{f_*}
        \end{bmatrix} 
        &,
        \begin{bmatrix}
        K(x, x) & K(x, x_*) \\
        K(x_*, x) & K(x_*, x_*)
        \end{bmatrix} 
        )
    \end{align} %]]></script>

<p>Where <script type="math/tex">K</script> is the Kernel function. We want to find the conditional distribution <script type="math/tex">f_* \vert f</script> and we proceed as outlined in Section 2.3 of <a href="https://www.amazon.co.uk/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?crid=FXBEX82B1MIO&amp;dchild=1&amp;keywords=pattern+recognition+and+machine+learning&amp;qid=1598177899&amp;sprefix=pattern+recognition%2Caps%2C147&amp;sr=8-1">Pattern Recognition and Machine Learning</a> (Bishop, 2006) and obtain the parameters of the conditional distribution:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
        p(f_* \vert f)
        &\sim \mathcal{N}( \mu_{f_* \vert f}, \Sigma_{f_* \vert f})
    \end{align} %]]></script>

<script type="math/tex; mode=display">\begin{align}
        \mu_{f_* \vert f} = \mu_{f_*} + K(x_*, x) K(x, x)^{-1} (f - \mu_f)
    \end{align}</script>

<script type="math/tex; mode=display">\begin{align}
        \Sigma_{f_* \vert f} = K(x_*, x_*) - K(x_*, x) K(x, x)^{-1} K(x, x_*)
    \end{align}</script>

<p>However, this assumes that training data contains non-noisy observations. In both the real world and in out generated dataste, we consider observations of the form <script type="math/tex">y = f(x) + \epsilon</script> with noise variance <script type="math/tex">0.4^2</script>. Noise is iid, therefore the variance of <script type="math/tex">x</script>,  <script type="math/tex">var(x) = cov(x, x) = K(x, x) + \sigma_n^2 I</script>. This leads to a change in the parameters of the conditional distribution <script type="math/tex">f_* \vert f</script>:</p>

<script type="math/tex; mode=display">\begin{align}
        \mu_{f_* \vert f} = \mu_{f_*} + K(x_*, x) (K(x, x) + \sigma_n^2 I)^{-1} (f - \mu_f)
    \end{align}</script>

<script type="math/tex; mode=display">\begin{align}
        \Sigma_{f_* \vert f} = K(x_*, x_*) - K(x_*, x) (K(x, x) + \sigma_n^2 I)^{-1} K(x, x_*)
    \end{align}</script>

<p>We take the mean function to be zero, leading to <script type="math/tex">\mu_f = \mu_{f_*} = 0</script>.</p>

<p>We now write a function to compute the covariance kernels.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre>    <span class="k">def</span> <span class="nf">get_covariance_matrices</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_star</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_function</span><span class="o">=</span><span class="n">kernel_squared_exponential</span><span class="p">):</span>
        <span class="s">"""
        K = K(x, x)
        K_s = K(x*, x)
        K_ss = K(x*, x*)
        """</span>
        
        <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_s</span> <span class="o">=</span> <span class="n">x_star</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kernel_function</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="n">sigma_f</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">K_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kernel_function</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="n">sigma_f</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_star</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">K_ss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kernel_function</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="n">sigma_f</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_star</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">x_star</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">K</span><span class="p">,</span> <span class="n">K_s</span><span class="p">,</span> <span class="n">K_ss</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="generating-test-points-and-visualising">Generating Test Points and Visualising</h3>

<p>We generate 100 test points and select the parameters for the Squared Exponential kernel, choosing <script type="math/tex">l = 0.1, \sigma_f = 1</script>. We then proceed to compute the covariance matrices for the training and test data.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre>    <span class="c1"># Generate some test data points
</span>
    <span class="n">n_star</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">x_star</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_star</span><span class="p">)</span>

    <span class="n">f_x_star</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_star</span><span class="p">)</span>

    <span class="c1"># Choose kernel functions parameters
</span>    <span class="n">sigma_f</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">l</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="c1">#alpha = 1
</span>
    <span class="c1"># Get the covariance matrices
</span>    <span class="n">K</span><span class="p">,</span> <span class="n">K_s</span><span class="p">,</span> <span class="n">K_ss</span> <span class="o">=</span> <span class="n">get_covariance_matrices</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_star</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="n">sigma_f</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">kernel_function</span><span class="o">=</span><span class="n">kernel_squared_exponential</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>We can visualise the covariance matrices, looking at the covariance between various observations.</p>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/cov_vis1.png" alt="Covariance heatmap for $$ K(x,x), K(x,x_*) $$" />
<em>Covariance heatmap for <script type="math/tex">K(x,x), K(x,x_*)</script></em></p>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/cov_vis2.png" alt="Covariance heatmap for $$ K(x_*,x_*) $$" />
<em>Covariance heatmap for <script type="math/tex">K(x_*,x_*)</script></em></p>

<p>Finally, although of little practical interest, we can draw samples from the prior and look at these. The prior has the distribution <script type="math/tex">f_* \sim \mathcal{N}(0, K(x_*, x_*))</script> and we can sample from it as follows:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre>    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">15</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_star</span><span class="p">),</span> <span class="n">cov</span><span class="o">=</span><span class="n">K_ss</span><span class="p">)</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x_star</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)</span>

    <span class="c1"># Plot f(x)
</span>    <span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"f(x)"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'Samples from Prior Distribution'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/prior.png" alt="Samples from Prior" />
<em>Samples from Prior</em></p>

<h3 id="computing-the-posterior">Computing the Posterior</h3>

<p>With the core machinery in place, we can now compute the parameters of the posterior distribution. We compute both <script type="math/tex">\mu_{f_* \vert f}</script> and <script type="math/tex">\Sigma_{f_* \vert f}</script> as</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">f_star_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_s</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">sigma_n</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">f_star_cov</span> <span class="o">=</span> <span class="n">K_ss</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_s</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">sigma_n</span><span class="o">**</span><span class="mi">2</span><span class="p">)),</span> <span class="n">K_s</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>We draw samples from this distribution and plot.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre>    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
        <span class="c1"># Sample from posterior distribution. 
</span>        <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">f_star_mean</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">cov</span><span class="o">=</span><span class="n">f_star_cov</span><span class="p">)</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_star</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
        
    <span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'f(x)'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'training points'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">f'Samples of Posterior, sigma_f = </span><span class="si">{</span><span class="n">sigma_f</span><span class="si">}</span><span class="s"> and l = </span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/posterior1.png" alt="Samples from Posterior" />
<em>Samples from Posterior</em></p>

<p>We can also look at the uncertainty of the model by plotting the regions within <script type="math/tex">n</script> standard deviations. We plot the posterior with the 2 standard deviation boundary, which is the 95% confidence interval.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">f_star_cov</span><span class="p">.</span><span class="n">diagonal</span><span class="p">())</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_star</span><span class="p">,</span> <span class="n">f_star_mean</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">std</span><span class="p">,</span> <span class="n">f_star_mean</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">std</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"2 SD"</span><span class="p">);</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'f(x)'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x_star</span><span class="p">,</span> <span class="n">f_star_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Posterior mean'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'training points'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">f'Samples of Posterior, sigma_f = </span><span class="si">{</span><span class="n">sigma_f</span><span class="si">}</span><span class="s"> and l = </span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/posterior2.png" alt="Posterior with Confidence Interval" />
<em>Posterior with Confidence Interval</em></p>

<h3 id="straying-further-out">Straying Further Out</h3>

<p>Next, we look into how the Gaussian Process behaves when performing inference on data it has never seen before â€“ this has particular applications in time-series modelling. The paper <a href="http://www.robots.ox.ac.uk/~sjrob/Pubs/philTransA_2012.pdf">Gaussian Processes for Timeseries Modelling</a> tackles this in detail and is recommended reading for more information when dealing with time-series.</p>

<p>We look at how the model behaves, paying particular attention to the model uncertainty when we perform inference on data that is not in the region of the training set. Note that only the test data changes here, with training data in the region <script type="math/tex">[0, 2]</script>.</p>

<p class="mx-auto d-block"><img src="/assets/blog/tech_blog/2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes/posterior3.png" alt="Predicting on Out of Sample Data" />
<em>Predicting on Out of Sample Data</em></p>

<p>As expected, we see near-identical performance in the region <script type="math/tex">[0, 1]</script>. What is more interesting, however, are the uncertainty bounds â€“ as soon as we stray from the region covered by the training data, the model uncertainty blows up! This serves to highlight the power of Gaussian Processes, because, when a GP isnâ€™t sure it reflects this in the uncertainty.</p>

<p>There is so much more to Gaussian Processes. We can achieve so much more by merely making better choices of Kernel (or Kernel combinations) as well as choosing hyperparameters for each kernel. We can also extend Gaussian Processes to classification problems. Recommended reading for all this and more is the <a href="http://www.gaussianprocess.org/gpml/">GPML</a> book.</p>

      </article>

      
        <div class="blog-tags">
          Tags:
          
          
            <a href="/tags#tech">tech</a>
          
            <a href="/tags#machine learning">machine learning</a>
          
          
        </div>
      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  
    <a href="https://twitter.com/intent/tweet?text=Starting+out+with+Gaussian+Processes&url=http%3A%2F%2Flocalhost%3A4000%2F2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes%2F"
      class="btn btn-social-icon btn-twitter" title="Share on Twitter">
      <span class="fab fa-fw fa-twitter" aria-hidden="true"></span>
      <span class="sr-only">Twitter</span>
    </a>
  

  
    <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes%2F"
      class="btn btn-social-icon btn-facebook" title="Share on Facebook">
      <span class="fab fa-fw fa-facebook" aria-hidden="true"></span>
      <span class="sr-only">Facebook</span>
    </a>
  

  
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2020-08-25-tech-blog-Starting-out-with-Gaussian-Processes%2F"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fab fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

  

</section>



      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2020-07-09-tech-blog-A-Simple-Trading-Strategy/" data-toggle="tooltip" data-placement="top" title="A Simple Trading Strategy">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2020-09-01-tech-blog-Relative-Momentum-Trading-Strategy/" data-toggle="tooltip" data-placement="top" title="Relative (Cross-Sectional) Momentum Trading Strategy">Next Post &rarr;</a>
        </li>
        
      </ul>
              
  
  
  

  



    </div>
  </div>
</div>


    <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:padmanaba.srinivasan16@imperial.ac.uk" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/dangerbot3pic" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/padmanaba-srinivasan-b67bb1137" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://steamcommunity.com/id/dangerbot3pic" title="Steam">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-steam fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Steam</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Padmanaba Srinivasan
        &nbsp;&bull;&nbsp;
      
      2021

      

      
      </p>
      <!-- Please don't remove this, keep my open source work credited :) -->
      <p class="theme-by text-muted">
        Theme by
        <a href="https://beautifuljekyll.com">beautiful-jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>

  
    
  
    
  <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/main.js"></script>
    
  






  
  </body>
</html>
